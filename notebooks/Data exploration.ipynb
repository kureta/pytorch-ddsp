{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodules.components.audio_dataset import AudioDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331573e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset('../data/preprocessed/cello/features.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e87ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "loudness = torch.cat([l['loudness'][0] for l in dataset.features]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "\n",
    "n, bins, patches = plt.hist(loudness, 128)\n",
    "plt.title(\"Loudness Histogram\")\n",
    "plt.xlabel(\"Db\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "l_min = loudness.min()\n",
    "l_max = loudness.max()\n",
    "mean = loudness.mean()\n",
    "std = loudness.std()\n",
    "start = mean - std\n",
    "end = mean + std\n",
    "\n",
    "plt.xticks([-70.0, -65.0, -60.0, -30.0, mean, l_min, l_max, start, end, start-std, end+std])\n",
    "plt.grid(axis='x')\n",
    "\n",
    "plt.axvline(x=l_min, linewidth=2, label=f'min={l_min:.2f}', color='k')\n",
    "plt.axvline(x=l_max, linewidth=2, label=f'max={l_max:.2f}', color='k')\n",
    "plt.axvline(x=mean, linewidth=2, label=f'mean={mean:.2f}', color='k', linestyle='dashed')\n",
    "plt.axvline(x=start, linewidth=2, label=f'-sigma={start:.2f}', color='g', linestyle='dashed')\n",
    "plt.axvline(x=end, linewidth=2, label=f'+sigma={end:.2f}', color='g', linestyle='dashed')\n",
    "plt.axvline(x=start-std, linewidth=2, label=f'-2sigma={start-std:.2f}', color='y', linestyle='dashed')\n",
    "plt.axvline(x=end+std, linewidth=2, label=f'+2sigma={end+std:.2f}', color='y', linestyle='dashed')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bcb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu102\n",
    "\n",
    "n, bins, patches = plt.hist(norm.cdf((loudness - mean) / std), 128)\n",
    "plt.title(\"Loudness Histogram\")\n",
    "plt.xlabel(\"Db\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d93134",
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = torch.cat([l['f0'][0] for l in dataset.features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dba620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bins_to_cents(bins):\n",
    "    \"\"\"Converts pitch bins to cents\"\"\"\n",
    "    cents = 20 * bins + 1997.3794084376191\n",
    "\n",
    "    # Trade quantization error for noise\n",
    "    return cents\n",
    "\n",
    "def cents_to_frequency(cents):\n",
    "    \"\"\"Converts cents to frequency in Hz\"\"\"\n",
    "    return 10 * 2 ** (cents / 1200)\n",
    "\n",
    "def freqs_to_cents(freq):\n",
    "    return 1200 * torch.log2(freq / 10.)\n",
    "\n",
    "def cents_to_bins(cents):\n",
    "    return (cents - 1997.3794084376191) / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = cents_to_bins(freqs_to_cents(f0)) / 359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = f0.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4392156",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "\n",
    "n, bins, patches = plt.hist(f0, 360)\n",
    "plt.title(\"F0 Histogram\")\n",
    "plt.xlabel(\"Db\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "l_min = f0.min()\n",
    "l_max = f0.max()\n",
    "mean = f0.mean()\n",
    "std = f0.std()\n",
    "start = mean - std\n",
    "end = mean + std\n",
    "\n",
    "plt.xticks([mean, l_min, l_max, start, end, start-std, end+std])\n",
    "plt.grid(axis='x')\n",
    "\n",
    "plt.axvline(x=l_min, linewidth=2, label=f'min={l_min:.2f}', color='k')\n",
    "plt.axvline(x=l_max, linewidth=2, label=f'max={l_max:.2f}', color='k')\n",
    "plt.axvline(x=mean, linewidth=2, label=f'mean={mean:.2f}', color='k', linestyle='dashed')\n",
    "plt.axvline(x=start, linewidth=2, label=f'-sigma={start:.2f}', color='g', linestyle='dashed')\n",
    "plt.axvline(x=end, linewidth=2, label=f'+sigma={end:.2f}', color='g', linestyle='dashed')\n",
    "plt.axvline(x=start-std, linewidth=2, label=f'-2sigma={start-std:.2f}', color='y', linestyle='dashed')\n",
    "plt.axvline(x=end+std, linewidth=2, label=f'+2sigma={end+std:.2f}', color='y', linestyle='dashed')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b25735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80886042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4906f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=251):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ninp=512, nhead=2, nhid=200, nlayers=2, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.ninp = ninp\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shit = TransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bok = torch.randn(5, 10, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e7d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = shit(bok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aca197",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d18fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.functional as AF\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f18f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.crepe_loss import CrepeLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38698565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loudness(waveform: Tensor, sample_rate: int):\n",
    "    if waveform.size(-2) > 5:\n",
    "        raise ValueError(\"Only up to 5 channels are supported.\")\n",
    "\n",
    "    gate_duration = 0.4\n",
    "    overlap = 0.75\n",
    "    gamma_abs = -70.0\n",
    "    kweight_bias = -0.691\n",
    "    gate_samples = int(round(gate_duration * sample_rate))\n",
    "    step = int(round(gate_samples * (1 - overlap)))\n",
    "\n",
    "    # Apply K-weighting\n",
    "    waveform = treble_biquad(waveform, sample_rate, 4.0, 1500.0, 1 / math.sqrt(2))\n",
    "    waveform = highpass_biquad(waveform, sample_rate, 38.0, 0.5)\n",
    "\n",
    "    # Compute the energy for each block\n",
    "    energy = torch.square(waveform).unfold(-1, gate_samples, step)\n",
    "    energy = torch.mean(energy, dim=-1)\n",
    "\n",
    "    # Compute channel-weighted summation\n",
    "    g = torch.tensor([1.0, 1.0, 1.0, 1.41, 1.41], dtype=waveform.dtype, device=waveform.device)\n",
    "    g = g[: energy.size(-2)]\n",
    "\n",
    "    energy_weighted = torch.sum(g.unsqueeze(-1) * energy, dim=-2)\n",
    "    loudness = -0.691 + 10 * torch.log10(energy_weighted)\n",
    "\n",
    "    # Apply absolute gating of the blocks\n",
    "    gated_blocks = loudness > gamma_abs\n",
    "    gated_blocks = gated_blocks.unsqueeze(-2)\n",
    "\n",
    "    energy_filtered = torch.sum(gated_blocks * energy, dim=-1) / torch.count_nonzero(gated_blocks, dim=-1)\n",
    "    energy_weighted = torch.sum(g * energy_filtered, dim=-1)\n",
    "    gamma_rel = kweight_bias + 10 * torch.log10(energy_weighted) - 10\n",
    "\n",
    "    # Apply relative gating of the blocks\n",
    "    gated_blocks = torch.logical_and(gated_blocks.squeeze(-2), loudness > gamma_rel.unsqueeze(-1))\n",
    "    gated_blocks = gated_blocks.unsqueeze(-2)\n",
    "\n",
    "    energy_filtered = torch.sum(gated_blocks * energy, dim=-1) / torch.count_nonzero(gated_blocks, dim=-1)\n",
    "    energy_weighted = torch.sum(g * energy_filtered, dim=-1)\n",
    "    LKFS = kweight_bias + 10 * torch.log10(energy_weighted)\n",
    "    return LKFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1968c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, amp, audio = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920967f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a48175",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = audio.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc58602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amp(example):\n",
    "    example = rearrange(example, \"b c t -> (b c) t\")\n",
    "    example = torch.nn.functional.pad(example, (19200 // 2, 19200 // 2))\n",
    "    example = example.unfold(1, 19200, 3 * 256)\n",
    "    example = rearrange(example, \"b c t -> (b c) t\").unsqueeze(1)\n",
    "\n",
    "    amp = AF.loudness(example, 48000)\n",
    "\n",
    "    return amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2641fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shit = get_amp(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(shit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bdff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AF.resample(audio, 48000, 16000).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = rearrange(audio, \"b c t -> (b c) t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a20046",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torch.nn.functional.pad(audio, (19200 // 2, 19200 // 2)).unfold(1, 19200, 3 * 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb373cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = rearrange(audio, \"bc f t -> (bc f) t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape, amp.shape, audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "crap = CrepeLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb79600",
   "metadata": {},
   "outputs": [],
   "source": [
    "crap.crepe.cuda()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ac436",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = audio.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shit = crap.loss(audio.cuda(), audio.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a60cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
